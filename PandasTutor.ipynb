{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3G6zmXDVs6k3bv6JMJoPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siripr4/pandas-tutor/blob/main/PandasTutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w8ihLkwdLqRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and run the Ollama API which is available at available at 0.0.0.0:11434\n",
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiiyXeqeNeQU",
        "outputId": "e026482c-fa6f-4ba9-d8ef-4977ba66db37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n",
            "100 11868    0 11868    0     0  34999      0 --:--:-- --:--:-- --:--:-- 35008\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A script to run the Ollama API in a separate thread so that the notebook's main thread is not blocked"
      ],
      "metadata": {
        "id": "WWpb7qsVRr2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "P2GrxOLtJVF7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model\n",
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.1:8b\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "dV6hDQ7xHpne"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNz7gMzVT0kk",
        "outputId": "8d06e9d6-dafd-4560-de2d-1fa10850bb7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from ollama) (0.27.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "ollama.pull(model='llama3.1:8b')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4WeS5VmRpCB",
        "outputId": "66efe12e-2c0f-49dc-a372-ea2cd38cb34a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'success'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = ollama.generate(model='llama3.1:8b', prompt='42 is the answer to the universe because...')\n",
        "generated['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Wn8S_Y6rV6kR",
        "outputId": "439d2d83-7358-43f4-af3c-a3bd87cf5271"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You\\'re referencing Douglas Adams\\' science fiction series \"The Hitchhiker\\'s Guide to the Galaxy\"!\\n\\nIn the book, a supercomputer named Deep Thought is asked to calculate the \"Answer to the Ultimate Question of Life, the Universe, and Everything.\" After 7.5 million years of computation, Deep Thought finally reveals that the answer is... 42.\\n\\nHowever, the characters in the story then realize that they don\\'t actually know what the ultimate question is, so the answer of 42 is essentially meaningless!\\n\\nThe series uses this concept to poke fun at the idea of a simple answer or solution to life\\'s mysteries. The number 42 has since become a cultural reference point and meme, symbolizing the absurdity and complexity of trying to find definitive answers in life.\\n\\nSo, that\\'s the \"reason\" behind 42 being the answer to the universe!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-n84_vnFqlr",
        "outputId": "63c472f4-1cd1-468a-ab28-0fc4de7238fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add functions to abstract ollama's chat API"
      ],
      "metadata": {
        "id": "tl6JR_9wLFHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from ollama import AsyncClient\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested use of asyncio.run\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Asynchronous function to handle the chat interaction\n",
        "async def _chat_interaction(content):\n",
        "    message = {'role': 'user', 'content': content}\n",
        "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
        "    return response\n",
        "\n",
        "# Synchronous function to run the asynchronous chat interaction\n",
        "def chat(content):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    if loop.is_running():\n",
        "        # If the event loop is already running, create a new task and wait for it\n",
        "        future = asyncio.ensure_future(_chat_interaction(content))\n",
        "        return loop.run_until_complete(future)\n",
        "    else:\n",
        "        return loop.run_until_complete(_chat_interaction(content))"
      ],
      "metadata": {
        "id": "XnEL3hSqFv5N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  content = \"Why is the sky blue?\"\n",
        "  response = chat(content)\n",
        "  print(response['message'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WtockukbLTjR",
        "outputId": "95937e5b-d41a-409c-8b92-46265a1ab45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<_chat_interaction() done, defined at <ipython-input-9-a81e98bea7f2>:9> exception=RemoteProtocolError('Server disconnected without sending a response.')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 373, in handle_async_request\n",
            "    resp = await self._pool.handle_async_request(req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\", line 216, in handle_async_request\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\", line 196, in handle_async_request\n",
            "    response = await connection.handle_async_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
            "    return await self._connection.handle_async_request(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 143, in handle_async_request\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 113, in handle_async_request\n",
            "    ) = await self._receive_response_headers(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 186, in _receive_response_headers\n",
            "    event = await self._receive_event(timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\", line 238, in _receive_event\n",
            "    raise RemoteProtocolError(msg)\n",
            "httpcore.RemoteProtocolError: Server disconnected without sending a response.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"<ipython-input-9-a81e98bea7f2>\", line 11, in _chat_interaction\n",
            "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ollama/_client.py\", line 653, in chat\n",
            "    return await self._request_stream(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ollama/_client.py\", line 517, in _request_stream\n",
            "    response = await self._request(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ollama/_client.py\", line 482, in _request\n",
            "    response = await self._client.request(method, url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 1574, in request\n",
            "    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 1661, in send\n",
            "    response = await self._send_handling_auth(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 1689, in _send_handling_auth\n",
            "    response = await self._send_handling_redirects(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 1726, in _send_handling_redirects\n",
            "    response = await self._send_single_request(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 1763, in _send_single_request\n",
            "    response = await transport.handle_async_request(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 372, in handle_async_request\n",
            "    with map_httpcore_exceptions():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.RemoteProtocolError: Server disconnected without sending a response.\n",
            "/usr/lib/python3.10/asyncio/locks.py:168: RuntimeWarning: coroutine 'chat_interaction' was never awaited\n",
            "  super().__init__(loop=loop)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8b97c84c57fe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Why is the sky blue?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a81e98bea7f2>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# If the event loop is already running, create a new task and wait for it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_chat_interaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_chat_interaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation"
      ],
      "metadata": {
        "id": "UqkRBofVbFZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Unzip the HTML Files\n",
        "zip_path = 'pandas.zip'\n",
        "extracted_path = 'extracted_html_files'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# Step 2: Parse Each HTML File and Generate QA Pairs\n",
        "data = {'question': [], 'answer': []}\n",
        "\n",
        "for root, dirs, files in os.walk(extracted_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.html'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                soup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "                # Extract relevant sections\n",
        "                for section in soup.find_all(['section', 'div']):  # Adjust tags as needed\n",
        "                    heading = section.find(['h2', 'h3'])\n",
        "                    if heading:\n",
        "                        content = section.get_text(separator=\"\\n\").strip()\n",
        "\n",
        "                        # \"What is\" questions\n",
        "                        what_question = f\"What is {heading.text}?\"\n",
        "                        data['question'].append(what_question)\n",
        "                        data['answer'].append(content)\n",
        "\n",
        "                        # \"How to\" questions if code snippets are present\n",
        "                        code_snippets = section.find_all('code')\n",
        "                        if code_snippets:\n",
        "                            how_question = f\"How to use {heading.text} in pandas?\"\n",
        "                            how_answer = \"\\n\".join([code.get_text() for code in code_snippets])\n",
        "                            data['question'].append(how_question)\n",
        "                            data['answer'].append(how_answer)\n",
        "\n",
        "# Step 3: Save as QA Pairs\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('pandas_qa_dataset.csv', index=False)\n",
        "\n",
        "print(\"QA pairs have been generated and saved to 'pandas_qa_dataset.csv'\")"
      ],
      "metadata": {
        "id": "1unnO-oCNJz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune the model using the dataset created above"
      ],
      "metadata": {
        "id": "KzTRnKSHnk0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate"
      ],
      "metadata": {
        "id": "d8A1HPRMqOBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyarrow\n",
        "!pip install --upgrade --force-reinstall datasets\n",
        "!pip install numpy==1.26.4\n",
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DkL9U1BnqQm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Load dataset\n",
        "dataset = Dataset.from_pandas(pd.read_csv('pandas_qa_dataset.csv'))\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['question'], examples['answer'], padding='max_length', truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    eval_dataset=tokenized_datasets\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YZ9r1ol6nkj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}